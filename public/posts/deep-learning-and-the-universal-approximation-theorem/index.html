<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  










  



  
  

<link rel="stylesheet" href="/scss/main.min.css" />


  
<meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
    <meta name="bingbot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
  

<title>Deep learning and the universal approximation theorem</title>


<meta name="author" content="">

<meta name="description" content="">
<link rel="canonical" href="/posts/deep-learning-and-the-universal-approximation-theorem/">
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep learning and the universal approximation theorem">
<meta property="og:description" content="Deep learning, and in particular deep neural networks, is one of the most popular and powerful tools of modern data science. Using analogies from neuroscience, neural networks provide a framework for building predictive models whose predictive power is seemingly limitless. They are at the heart of modern &ldquo;artificial intelligence&rdquo; wonders like smart assistants (Siri, Cortana, Google Assistant), smart speakers, self-driving cars and most modern image detection algorithms (Google image search, IBM Watson).">
<meta property="og:url" content="/posts/deep-learning-and-the-universal-approximation-theorem/">









<meta name="twitter:title" content="Deep learning and the universal approximation theorem">
<meta name="twitter:description" content="Deep learning, and in particular deep neural networks, is one of the most popular and powerful tools of modern data science. Using analogies from neuroscience, neural networks provide a framework for building predictive models whose predictive power is seemingly limitless. They are at the heart of modern &ldquo;artificial intelligence&rdquo; wonders like smart assistants (Siri, Cortana, Google Assistant), smart speakers, self-driving cars and most modern image detection algorithms (Google image search, IBM Watson).">



  <meta name="twitter:card" content="summary">






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": "Person",
      "@id": "/#/schema/person/1",
      "name":  null ,
      "url": "/",
      "image": {
        "@type": "ImageObject",
        "@id": "/#/schema/image/1",
        "url": "/\u003cnil\u003e",
        "width": null ,
        "height": null ,
        "caption":  null 
      }
    },
    {
      "@type": "WebSite",
      "@id": "/#/schema/website/1",
      "url": "/",
      "name": "Clinton Boys",
      "description":  null ,
      "publisher": {
        "@id": "/#/schema/person/1"
      }
    },
    {
      "@type": "WebPage",
      "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/",
      "url": "/posts/deep-learning-and-the-universal-approximation-theorem/",
      "name": "Deep learning and the universal approximation theorem",
      "description":  null ,
      "isPartOf": {
        "@id": "/#/schema/website/1"
      },
      "about": {
        "@id": "/#/schema/person/1"
      },
      "datePublished": "0001-01-01T00:00:00+00:00",
      "dateModified": "0001-01-01T00:00:00+00:00",
      "breadcrumb": {
        "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/#/schema/breadcrumb/1"
      },
      "primaryImageOfPage": {
        "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/#/schema/image/2"
      },
      "inLanguage":  null ,
      "potentialAction": [{
        "@type": "ReadAction", "target": ["/posts/deep-learning-and-the-universal-approximation-theorem/"]
      }]
    },
    {
      "@type": "BreadcrumbList",
      "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/#/schema/breadcrumb/1",
      "name": "Breadcrumbs",
      "itemListElement": [{
        "@type": "ListItem",
        "position":  1 ,
        "item": {
          "@type": "WebPage",
          "@id": "",
          "url": "",
          "name": "Home"
          }
        },{
        "@type": "ListItem",
        "position":  3 ,
        "item": {
          "@type": "WebPage",
          "@id": "/posts/",
          "url": "/posts/",
          "name": "Posts"
          }
        },{
        "@type": "ListItem",
        "position":  4 ,
        "item": {
          "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/"
          }
        }]
    },
    {
      "@context": "https://schema.org",
      "@graph": [
        {
          "@type": "Article",
          "@id": "/#/schema/article/1",
          "headline": "Deep learning and the universal approximation theorem",
          "description": "",
          "isPartOf": {
            "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/"
          },
          "mainEntityOfPage": {
            "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/"
          },
          "datePublished": "0001-01-01T00:00:00+00:00",
          "dateModified": "0001-01-01T00:00:00+00:00",
          "author": {
            "@id": "/#/schema/person/1"
          },          
          "publisher": {
            "@id": "/#/schema/person/1"
          },
          "image": {
            "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/#/schema/image/2"
          }
        }
      ]
    },{
      "@context": "https://schema.org",
      "@graph": [
        {
          "@type": "ImageObject",
          "@id": "/posts/deep-learning-and-the-universal-approximation-theorem/#/schema/image/2",
          "url":  null ,
          "contentUrl":  null ,
          "caption": "Deep learning and the universal approximation theorem"
        }
      ]
    }
  ]
}
</script>
  

  

</head><body>
    <header class="container">
  <nav class="main-nav" id="js-navbar">
    <a class="logo" href="">Clinton Boys</a>
    <ul class="menu" id="js-menu">
      
      
      
        <li class="menu-item">
          <a href="/about/" class="menu-link">About</a>
        </li>
      
      
      
        <li class="menu-item">
          <a href="http://mtsolitary.com" class="menu-link">Digital Garden</a>
        </li>
      
      
      
        <li class="menu-item">
          <a href="/now/" class="menu-link">Now</a>
        </li>
      
      
      
        <li class="menu-item">
          <a href="/projects/" class="menu-link">Projects</a>
        </li>
      
      
      
        <li class="menu-item">
          <a href="/writing/" class="menu-link">Writing</a>
        </li>
      
      
      <li class="menu-item--align">
        <div class="switch">
          <input class="switch-input" type="checkbox" id="themeSwitch">
          <label aria-hidden="true" class="switch-label" for="themeSwitch">On</label>
          <div aria-hidden="true" class="switch-marker"></div>
        </div>
      </li>
    </ul>
    <span class="nav-toggle" id="js-navbar-toggle">
      <svg xmlns="http://www.w3.org/2000/svg" id="Outline" viewBox="0 0 24 24" width="30" height="30" fill="var(--color-contrast-high)"><rect y="11" width="24" height="2" rx="1"/><rect y="4" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg>
    </span>
  </nav>
</header><main class="section">
<div class="container">
  <section class="page-header">
    <h1 class="page-header-title">Deep learning and the universal approximation theorem</h1>
    <div class="post-list-meta">
      <div class="post-list-dates">Jan 1, 0001&nbsp;&middot;&nbsp;4 min.</div>
      
      
    </div>
    <p class="page-header-desc"></p>
    <div class="single-terms">
      
    </div>
  </section>
</div>
<div class="single-container-post">
  

  <div class="single-post-contents">
    <div class="single-feature-img">



  

</div>
    <article class="markdown">
        <p>Deep learning, and in particular deep neural networks, is one of the most popular and
powerful tools of modern data science. Using analogies from neuroscience, neural networks
provide a framework for building predictive models whose predictive power is seemingly
limitless. They are at the heart of modern &ldquo;artificial intelligence&rdquo; wonders like smart
assistants (Siri, Cortana, Google Assistant), smart speakers, self-driving cars and most
modern image detection algorithms (Google image search, IBM Watson).</p>
<p>This post will give a mathematically-slanted overview to neural networks, with the goal
of proving what could be referred to as the &ldquo;Fundamental Theorem of Neural Networks&rdquo;. This
remarkable theorem is fairly accessible, and explains beautifully and precisely why neural
networks are so powerful.</p>
<h2 id="the-mathematics-of-neural-networks"></h>The mathematics of neural networks<a href="#the-mathematics-of-neural-networks">
    
  </a>
</h2><p>A neural network is essentially a function approximation algorithm, modelled at a basic level
on the behaviour of neurons in the human brain. Although it is not a particularly common
way of thinking as a data scientist, from a mathematical perspective machine learning
is nothing more than function approximation with a probabilistic flavour. Under the assumption
that the &ldquo;true&rdquo; relationship between a set of features $$X$$ and an output variable $$y$$ can be specified
by a function $$f(X, \theta)$$ which may depend further on some parameters $$\theta$$, the problem
of machine learning can be expressed as an attempt to approximate $$f$$ in a way which minimizes
a specified cost function (the choice of cost function is usually motivated probabilistically).</p>
<p>For a given input $$X\in\mathbb{R}^n$$, a <strong>neural network with a single hidden layer</strong> is a finite linear combination of the form</p>
<p>$$
\sum_{j=1}^N\alpha_j\sigma(y_j^TX+\theta_j)
$$</p>
<p>where $$N,\alpha_j\in \mathbb{R}$$, $$y_j\in\mathbb{R}^n$$ and $$\sigma$$ is a special type of non-linear function we will discuss later. The $$\alpha_j$$&rsquo;s are called the <em>weights</em>, the $$\theta_j$$ the <em>biases</em> and $$\sigma$$ is called the <em>activation function</em>.</p>
<p><img src="https://github.com/clintonboys/clintonboys.github.io/blob/master/_posts/nn.png?raw=true"
	alt="Neural network" 
	class="single-post-image" 
	loading="lazy"
	decoding="async"
	></p>
<p>The above picture explains the analogy with the human brain; given an input vector $X$ (touch input from skin, audio input from the ears, etc.), signals are sent to various
neurons which &ldquo;activate&rdquo; according to these inputs and a collection of weights. This behaviour then cascades through
various layers of the network, resulting finally in an output (don&rsquo;t touch the fire, say &ldquo;yes&rdquo;, etc.).</p>
<p>We will be less concerned in this article with the &ldquo;machine learning&rdquo; theory behind neural networks &ndash; for
example I don&rsquo;t want to discuss the algorithms which are used to train the networks, or techniques for
feature selection or choosing the right activation function or number of &ldquo;hidden layers&rdquo;. Rather, we will
discuss a mathematical theorem which provides a solid theoretical basis for why neural networks are so powerful.</p>
<h2 id="the-universal-approximation-theorem"></h>The universal approximation theorem<a href="#the-universal-approximation-theorem">
    
  </a>
</h2><p>Universality theorems are omnipresent in mathematics. The setup is classic, and gives the illusion of a genius mind
at work, inventing a solution from thin air and then proceeding to prove it is the unique one which suits
a given set of circumstances. In actuality, the <em>workflow</em> is entirely opposite: first a family of functions or a
mathematical object is studied by its occurrences in specific examples. Through careful study of these examples,
its utility becomes clear and the question of generalisation and universality arises.</p>
<p>So seeing the above family of functions occurring in many different places, the natural question is: what can we use
them for? The answer turns out to be just about everything: in a way which can be made mathematically precise,
neural networks with a single layer can be used to approximate any continuous function. Let&rsquo;s state this theorem a
little more clearly. The following theorem comes from a <a href="https://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf" target="_blank" rel="noopener">paper</a> of Cybenko. A <strong>sigmoidal</strong> function is one which &ldquo;looks&rdquo; like a sigmoid: $$\sigma(x)\to1$$ as $$x\to\infty$$ and $$\sigma(x)\to0$$ as $$x\to-\infty$$.</p>
<hr>
<p><strong>Universal approximation theorem for neural networks (Cybenko)</strong></p>
<p>Let $$\sigma$$ be any continuous sigmoidal function. Then finite sums of the form</p>
<p>$$
G(x) = \sum_{j=1}^N \alpha_j\sigma(y_j^Tx + \theta_j)
$$</p>
<p>are dense in the set $$C(I_n)$$ of continuous functions on the unit cube.</p>
<hr>
<p>For those who don&rsquo;t remember their undergraduate analysis, there is a theoretical definition of
density <a href="https://en.wikipedia.org/wiki/Dense_set" target="_blank" rel="noopener">here</a>, but for our purposes it is enough to think of it
as meaning that there are &ldquo;enough&rdquo; of these functions to be able to use them as &ldquo;building blocks&rdquo; to build
any function to any required level of accuracy.</p>
<p>The proof of this remarkable theorem uses fairly standard functional analysis techniques. First we show that
a slightly weaker but easier-to-work with family of functions (called <strong>discriminatory</strong> functions)
satisfy the conditions of the theorem. Then it proceeds to prove, using a standard functional analysis method,
that sigmoidal functions belong to this category.</p>

    </article>
    <aside>
      <div class="single-terms">
        
      </div>
      
        

        
  <section>
    <h2>Read Next</h2>
    <div class="single-next-previous">
      
        <a class="previous" href="/posts/deriving-keplers-laws-from-newtons-laws/">&laquo; Deriving Kepler&#39;s laws from Newton&#39;s laws</a>
      
      
        <a class="next" href="/posts/correcting-for-reliability-in-australian-election-polls/">Correcting for reliability in Australian election polls &raquo;</a>
      
    </div>
  </section>

      
    </aside>
  </div>
</div>

    </main><footer>
  
  <div class="section footer">
    <p class="footer-copyright"><i>&copy; 2014 - 2024
      Clinton Boys</i>
      
    </p>
    
  </div>
</footer>

  








  
  
    

<script src="/main.min.js"></script>



  
</body>
</html>
